Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Is Training
500,1.4200913,1.0,-67.69401,-122.94370524088542,-122.94370524088542,32362.344,0.3649602,0.00039479998,1.0
1000,1.425769,1.0,-72.25243,-1.000030517578125,-1.000030517578125,67482.945,0.3026505,0.00038699998,1.0
1500,1.4304407,1.0,-52.763836,-1.000030517578125,-1.000030517578125,31873.295,0.26275283,0.0003766,1.0
2000,1.4347068,1.0,-30.714458,-1.000030517578125,-1.000030517578125,14877.737,0.2668197,0.0003662,1.0
2500,1.4410077,1.0,-7.8134933,-1.000030517578125,-1.000030517578125,8175.2656,0.17895114,0.00035580003,1.0
3000,1.4444464,1.0,4.9059696,-1.000030517578125,-1.000030517578125,5206.339,0.18561241,0.00034539998,1.0
3500,1.4506915,1.0,4.664115,-1.000030517578125,-1.000030517578125,4191.431,0.19358838,0.000335,1.0
4000,1.4554534,1.0,1.8115147,-1.000030517578125,-1.000030517578125,3500.9211,0.23744091,0.00032460003,1.0
4500,1.4603581,1.0,1.7095653,-1.000030517578125,-1.000030517578125,3131.2373,0.2726917,0.0003142,1.0
5000,1.4644121,1.0,1.2201482,-1.000030517578125,-1.000030517578125,2843.8594,0.1504614,0.00030380004,1.0
5500,1.4678947,1.0,3.3317943,-1.000030517578125,-1.000030517578125,2567.3027,0.13079979,0.00029340002,1.0
6000,1.4700261,1.0,4.717698,-1.000030517578125,-1.000030517578125,2374.8909,0.14133926,0.000283,1.0
